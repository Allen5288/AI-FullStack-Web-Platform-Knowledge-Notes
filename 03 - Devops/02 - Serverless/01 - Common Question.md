### **1. How do you handle cold starts in AWS Lambda?**

ðŸ§  **Answer:**

> â€œMinimize cold start impact by keeping my functions small, reusing heavy resources outside the handler scope, and tuning memory/time settings. For critical APIs, I use provisioned concurrency.â€
>

ðŸ’¡ **Best Practices:**

- Move DB clients, vector stores, and model loading **outside** the handler.
- Avoid initializing large dependencies **inside** `handler()`.
- Set **512MB+ memory** â†’ helps reduce init time.
- Use **provisioned concurrency** for high-traffic endpoints.
- Optionally use `serverless-plugin-warmup`.

### **2. How do you manage environment configs and secrets across stages?**

ðŸ§  **Answer:**

> â€œStore all secrets like API keys or DB URIs in AWS SSM Parameter Store, and load them via environment variables in the Serverless Framework. Each stage (dev, prod, clinic-a) has its own isolated config.â€
>

ðŸ’¡ **Best Practices:**

- Use `serverless.yml` to inject values:

```yaml
environment:
  GEMINI_API_KEY: ${ssm:/ai/gemini/api-key}
```

- Use a naming convention: `/project/env/KEY_NAME`
- Donâ€™t hardcode secrets in code or `.env` files
- Load config in `config.ts` file:

```
export const GEMINI_KEY = process.env.GEMINI_API_KEY!;
```

- Use stages like `dev`, `staging`, `clinic-a`, `prod`

### **3. How does API Gateway integrate with Lambda?**

ðŸ’¡ **Details:**

- **API Gateway** can be:
  - **HTTP API** (faster, cheaper) âœ…
  - **REST API** (more features)
- Handles incoming requests â†’ transforms â†’ invokes Lambda

- Input mapping (headers, query, body) is auto-parsed by `middy` or `Fastify`
- Add CORS, throttling, auth (IAM, JWT) at API Gateway level

```yaml
functions:
  chat:
    handler: src/handlers/chat.handler
    events:
      - http:
          path: chat
          method: post
          cors: true
          private: true
```

### **4. How do you debug Lambda functions in production?**

ðŸ§  **Answer:**

> â€œUse structured logging with user/session IDs, and Enable CloudWatch logs for every function. Track latency and error rates using X-Ray and custom metrics.â€
>

ðŸ’¡ **Debugging Tools:**

- Use `console.log` with JSON structured logs
- Use correlation IDs (per user or request)
- `serverless logs -f <func> -t` to tail logs
- Add try/catch with clear error messages:

```jsx
try {
  await doSomething();
} catch (err) {
  log.error({ err, requestId }, "Chatbot failed");
  throw new InternalError("AI service failed");
}
```

- Use CloudWatch dashboards + X-Ray traces

### 5**: How do you write tests for your Node.js serverless APIs?**

ðŸ§  **Answer:**

> â€œUse Jest for unit and integration testing. Each handler and service is tested in isolation using mocks, and I use AWS SDK v3 mock tools for testing external dependencies like DynamoDB or S3. For end-to-end tests, I simulate Lambda locally with serverless invoke local.â€
>

ðŸ’¡ **Best Practices:**

- **Unit tests**: Services, utils â€” mock external calls
- **Integration tests**: Use local DynamoDB or test S3 bucket
- **Handler tests**: Test Lambda handler logic w/ mock events
- **Use AWS SDK mocks**: `aws-sdk-client-mock` for v3
- Simulate API Gateway input (event, context)

ðŸ“ Example Test Folder:

```
/tests
  /unit
    chatbotService.test.ts
  /integration
    chatHandler.test.ts

```

ðŸ”§ Tools:

- `jest`
- `supertest` (for Fastify-based handlers)
- `serverless invoke local`

---

### **6. How do you ensure good performance in your Node.js Lambda app?**

ðŸ§  **Answer:**

> â€œOptimize cold start by moving all reusable objects outside the handler scope and minimizing bundle size. I also tune memory and timeout, avoid heavy dependencies, and use caching when possible.â€
>

ðŸ’¡ **Performance Techniques:**

- Keep **handler thin** and logic modular
- Use **tree-shaking and bundlers** (e.g., esbuild, webpack)
- Avoid large packages (e.g., lodash â†’ use lodash-es or native)
- Tune **memory size**: 512MBâ€“1024MB is often faster
- Reduce dependencies â€” e.g., use AWS SDK v3 modular packages
- Use **TTL cache** (in-memory or Redis/S3/DynamoDB) for popular AI responses

ðŸ“¦ Example `serverless.yml` tuning:

```yaml
timeout: 30
memorySize: 768
```

---

### **7. How do you handle API versioning?**

ðŸ§  **Answer:**

> â€œVersion my APIs at the route level (e.g., /v1/chat), and sometimes by Lambda alias. For major changes, I deploy a separate version of the function so old clients can continue using v1 safely.â€
>

ðŸ’¡ **Versioning Strategies:**

- URI versioning: `/v1/chat`, `/v2/chat`
- Optional: Use Lambda aliases (`v1`, `v2`) mapped via API Gateway stage variables
- Include version in OpenAPI docs

ðŸ“ Optional routing structure:

```
/handlers
  /v1
    chat.ts
  /v2
    chat.ts
```

---

### **8. What are some best practices for designing serverless APIs?**

ðŸ§  **Answer:**

> â€œDesign APIs with clear route separation, strict input validation, role-based access control, and consistent error handling. I keep each Lambda single-purpose, and use shared middlewares for auth and logging.â€
>

ðŸ’¡ **API Design Best Practices:**

| Area | Practice |
| --- | --- |
| âœ… Routes | RESTful (e.g., `/appointments`, `/chat`) |
| âœ… Auth | Use JWT or Cognito authorizers |
| âœ… Input validation | Use `zod` or `class-validator` for schema validation |
| âœ… Errors | Consistent shape: `{ statusCode, message, type }` |
| âœ… Rate limit | API Gateway usage plans or Lambda throttling |
| âœ… Logging | Use correlation IDs, structured JSON logs |
| âœ… Monitoring | Track usage per endpoint / tenant |
| âœ… Throttling | Consider burst limits per user/clinic via API Gateway usage plans |

ðŸ“¦ Common Middleware (via `middy`):

- `httpErrorHandler`
- `httpJsonBodyParser`
- `validator`
- `cors`
- `logger`

ðŸ“¦ Input Validation Example:

```jsx
import { z } from 'zod';

const ChatSchema = z.object({
  message: z.string().min(1),
  userId: z.string().uuid()
});

function validate(input: any) {
  return ChatSchema.parse(input);
}
```
